[TASK]
train = True
test = True

[MODEL]
bert_model = deepset/gbert-large
bert_tokenizer = deepset/gbert-large
model_abbr = GBERTL

[DATA]
filepath_train = ./data/train.json
filepath_dev = ./data/dev.json
filepath_test =  ./data/test.json
result_folder = ./results


[PARAM]
optimizer = AdamW
epochs_train = 10
batch_size = 16
learning_rate = 2.693154582157772e-05
eps = 5.45374378277376e-07
weight_decay = 0.019840937077311938
gradient_clip = 1.0
num_warmup_steps = 0
print_info_every = 5 
seed = 18
run = 1


